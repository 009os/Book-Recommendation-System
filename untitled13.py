# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sM-Jse3jkBsHZNrDzIhcAKSKxsdDNxAT

# **Importing required libraries**

1. `pandas`: Pandas is a powerful data manipulation library in Python. In this code, it is used to load the dataset from a CSV file, preprocess it, and perform operations on the data, such as dropping unnecessary columns.

2. `sklearn.feature_extraction.text.TfidfVectorizer`: This module from scikit-learn is used to convert a collection of raw documents (in this case, book titles) to a matrix of TF-IDF features. TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents. This is useful for converting text data into a format that machine learning models can understand.

3. `sklearn.metrics.pairwise.cosine_similarity`: This module from scikit-learn computes the cosine similarity between samples in X and Y. Cosine similarity is a metric used to measure how similar two vectors are, in this case, the similarity between the query and each book title.

4. `sklearn.decomposition.TruncatedSVD`: This module from scikit-learn performs dimensionality reduction using truncated singular value decomposition (SVD). It is used here to reduce the dimensionality of the TF-IDF matrix.

5. `sklearn.pipeline.make_pipeline`: This function from scikit-learn constructs a pipeline from the given estimators. A pipeline is a sequence of data processing steps, typically including feature extraction, dimensionality reduction, and model training.

6. `sklearn.preprocessing.Normalizer`: This module from scikit-learn normalizes samples individually to unit norm. It is used here as part of the pipeline to normalize the data after dimensionality reduction.

7. `numpy`: NumPy is a fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. In this code, NumPy is used for various array operations and computations.

8. `matplotlib.pyplot`: Matplotlib is a plotting library for Python. The `pyplot` module provides a MATLAB-like interface for creating and customizing plots. In this code, it is used to create visualizations, such as bar plots to display recommended books and their publication years.
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
import numpy as np
import matplotlib.pyplot as plt
#Lets go for DataSet

"""# **Loading the Rough DATA SET**

"""

Books = pd.read_csv("Books.csv")
Books.head(3)

"""# **Cleaning our Data Set**
The line `Books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)` is a command in pandas that removes specific columns from a DataFrame. Hereâ€™s a detailed explanation of each part of this line:

1. **`Books.drop([...], axis=1, inplace=True)`**:
   - `Books`: This is the DataFrame that you are working with, which contains the dataset of books.
   - `drop([...])`: The `drop` method is used to remove specified labels from rows or columns.
   - `['Image-URL-S', 'Image-URL-M', 'Image-URL-L']`: This list specifies the columns that you want to remove from the DataFrame. These columns contain URLs of book images in different sizes (Small, Medium, Large).
   - `axis=1`: The `axis` parameter determines whether to drop labels from the rows or columns. `axis=0` would drop rows, while `axis=1` drops columns.
   - `inplace=True`: This parameter, when set to `True`, modifies the DataFrame in place, meaning the columns are removed directly from the existing DataFrame without creating a new one. If it were set to `False`, it would return a new DataFrame with the columns removed, leaving the original DataFrame unchanged.

### Why Drop These Columns?

- **Unnecessary for Analysis**: The columns `'Image-URL-S'`, `'Image-URL-M'`, and `'Image-URL-L'` contain URLs to images of the book covers in different sizes. These URLs are typically not useful for most types of data analysis or recommendation tasks, which usually focus on textual data like book titles, authors, and publishers.
- **Reducing Memory Usage**: Removing columns that are not needed for the analysis helps to reduce the memory footprint of the DataFrame. This can be particularly important when working with large datasets.
- **Simplifying the DataFrame**: Dropping irrelevant columns helps to simplify the DataFrame, making it easier to understand and work with the remaining data. This can also improve the performance of certain operations and algorithms that you might apply to the DataFrame.

### Example Before and After Dropping Columns

#### Before Dropping Columns

| Book-Title | Book-Author | Publisher       | Image-URL-S            | Image-URL-M            | Image-URL-L            |
|------------|-------------|-----------------|------------------------|------------------------|------------------------|
| Book1      | Author1     | Publisher1      | url_small_1            | url_medium_1           | url_large_1            |
| Book2      | Author2     | Publisher2      | url_small_2            | url_medium_2           | url_large_2            |

#### After Dropping Columns

| Book-Title | Book-Author | Publisher       |
|------------|-------------|-----------------|
| Book1      | Author1     | Publisher1      |
| Book2      | Author2     | Publisher2      |

In the context of a recommendation system, only the textual information about the books is usually needed to calculate similarities and generate recommendations. Hence, dropping the image URL columns helps to streamline the dataset for this purpose.


"""

# Drop unnecessary columns
Books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

Books.head(10)

"""# **TRAINING AND TESTING**
This code snippet performs the following tasks:

1. **Splitting the Data**: The dataset `Books` is split into training and testing sets using the `train_test_split` function from scikit-learn. The training set contains 80% of the data, and the testing set contains the remaining 20%. The `random_state=42` parameter ensures reproducibility by fixing the random seed.

2. **Preprocessing the Training Data**:
   - The book titles in the training data are preprocessed using a function called `preprocess`. This function likely performs text preprocessing steps such as lowercasing, removing punctuation, and possibly stemming or lemmatization.
   - The preprocessed book titles are then transformed into numerical features using the `TfidfVectorizer` from scikit-learn. This converts the text data into a matrix of TF-IDF features, which represents the importance of each word in each book title relative to the entire corpus of book titles.
   - Dimensionality reduction is applied to the TF-IDF matrix using Truncated Singular Value Decomposition (SVD) with 100 components. This reduces the dimensionality of the data while preserving important information.

3. **Preprocessing the Testing Data**:
   - The same preprocessing steps applied to the training data are also applied to the testing data. This ensures consistency in data processing between the training and testing sets.
   - The testing data is transformed using the same TF-IDF vectorizer fitted on the training data. This ensures that the same set of features is used for both training and testing.

4. **Dimensionality Reduction**:
   - The testing data is further transformed using the same dimensionality reduction pipeline (Truncated SVD and normalization) applied to the training data. This ensures that the testing data is represented in the same lower-dimensional space as the training data.

Overall, this code prepares the dataset for a machine learning task, where the goal is likely to train a model on the training data and evaluate its performance on the testing data. The specific machine learning algorithm and task (e.g., classification, clustering) are not included in this snippet but would typically follow these preprocessing steps.
Hence, lsa_loaded handles the semantic analysis of textual data, while vectorizer_loaded handles the preprocessing and vectorization of raw text data before feeding it into the LSA model. Both are essential components of the text processing pipeline for tasks such as document similarity, classification, or recommendation.

"""

import re
import string

def preprocess(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)
    return text

from sklearn.model_selection import train_test_split
import joblib



# Splitting the data into training and testing sets
train_data, test_data = train_test_split(Books, test_size=0.2, random_state=42)

# Preprocessing the training data
train_data['Book-Title'] = train_data['Book-Title'].apply(preprocess)
vectorizer = TfidfVectorizer(stop_words='english')
X_train = vectorizer.fit_transform(train_data['Book-Title'])
svd = TruncatedSVD(n_components=100)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)
X_train_lsa = lsa.fit_transform(X_train)

# Preprocessing the testing data
test_data['Book-Title'] = test_data['Book-Title'].apply(preprocess)
X_test = vectorizer.transform(test_data['Book-Title'])
X_test_lsa = lsa.transform(X_test)


# Save the model
joblib.dump(lsa, 'lsa_model.pkl')
joblib.dump(vectorizer, 'vectorizer.pkl')


# Loading the model
lsa_loaded = joblib.load('lsa_model.pkl')
vectorizer_loaded = joblib.load('vectorizer.pkl')

"""# **Data representation**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np

"""# **Pie Chart: Distribution of Books by Publisher**

### `Books.fillna('', inplace=True)`
- **Purpose**: This line fills missing (NaN) values in the DataFrame `Books` with an empty string `''`.
- **`.fillna('', inplace=True)`:** This method call fills missing values in-place with the specified value (`''`). Setting `inplace=True` modifies the DataFrame `Books` directly.

### `publisher_counts = Books['Publisher'].value_counts().head(10)`
- **Purpose**: This line calculates the counts of unique publishers in the 'Publisher' column of the `Books` DataFrame and selects the top 10 publishers.
- **`Books['Publisher'].value_counts()`:** This method returns a Series containing counts of unique values in the 'Publisher' column.
- **`.head(10)`:** This method selects the top 10 publishers with the highest counts.

### `plt.figure(figsize=(10, 8))`
- **Purpose**: This line creates a new figure for plotting with a specific size.
- **`figsize=(10, 8)`:** This parameter specifies the width and height of the figure in inches.

### `plt.pie(publisher_counts, labels=publisher_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)`
- **Purpose**: This line creates a pie chart to visualize the distribution of publishers.
- **`publisher_counts`:** The counts of publishers, which will be used to determine the size of each slice in the pie chart.
- **`labels=publisher_counts.index`:** The labels for each slice are set to the index of `publisher_counts`, which contains the names of the publishers.
- **`autopct='%1.1f%%'`:** This parameter specifies the format of the percentage display on each slice.
- **`startangle=140`:** This parameter sets the start angle for the first slice of the pie chart.
- **`colors=plt.cm.Paired.colors`:** This parameter sets the colors for the slices of the pie chart using the Paired color map from Matplotlib.

### `plt.title('Top 10 Publishers')`
- **Purpose**: This line sets the title of the pie chart.
- **`'Top 10 Publishers'`:** This string specifies the title of the pie chart.

### `plt.show()`
- **Purpose**: This line displays the pie chart.
- **`plt.show()`:** This function displays the current figure (pie chart) that was created earlier.

### Summary:
- The code fills missing values in the 'Publisher' column of the `Books` DataFrame with an empty string.
- It calculates the counts of unique publishers and selects the top 10 publishers.
- A pie chart is created to visualize the distribution of the top 10 publishers.
- The pie chart is displayed using Matplotlib's `plt.show()` function.
"""

# Fill missing values
Books.fillna('', inplace=True)

publisher_counts = Books['Publisher'].value_counts().head(10)  # Top 10 publishers
plt.figure(figsize=(10, 8))
plt.pie(publisher_counts, labels=publisher_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
plt.title('Top 10 Publishers')
plt.show()

"""# **Bar Chart: Top 10 Authors with Most Books**


### `author_counts = Books['Book-Author'].value_counts().head(10)`
- **Purpose**: This line calculates the counts of unique authors in the 'Book-Author' column of the `Books` DataFrame and selects the top 10 authors with the most books.
- **`Books['Book-Author'].value_counts()`:** This method returns a Series containing counts of unique values in the 'Book-Author' column, effectively counting how many books each author has.
- **`.head(10)`:** This method selects the top 10 authors with the highest counts.

### `plt.figure(figsize=(12, 6))`
- **Purpose**: This line creates a new figure for plotting with a specific size.
- **`figsize=(12, 6)`:** This parameter specifies the width and height of the figure in inches.

### `sns.barplot(x=author_counts.index, y=author_counts.values, color='purple')`
- **Purpose**: This line creates a bar plot to visualize the top 10 authors with the most books.
- **`x=author_counts.index`:** The x-axis of the bar plot represents the authors (index of `author_counts`), which are the top 10 authors.
- **`y=author_counts.values`:** The y-axis represents the number of books written by each author (values of `author_counts`).
- **`color='purple'`:** This parameter sets the color of the bars in the bar plot to purple.

### `plt.title('Top 10 Authors with Most Books')`
- **Purpose**: This line sets the title of the bar plot.
- **`'Top 10 Authors with Most Books'`:** This string specifies the title of the bar plot.

### `plt.xlabel('Author')`
- **Purpose**: This line sets the label for the x-axis of the bar plot.
- **`'Author'`:** This string specifies the label for the x-axis, indicating the authors.

### `plt.ylabel('Number of Books')`
- **Purpose**: This line sets the label for the y-axis of the bar plot.
- **`'Number of Books'`:** This string specifies the label for the y-axis, indicating the number of books.

### `plt.xticks(rotation=45)`
- **Purpose**: This line rotates the x-axis labels to improve readability.
- **`rotation=45`:** This parameter specifies the rotation angle for the x-axis labels. A rotation of 45 degrees is commonly used to prevent overlapping labels.

### `plt.show()`
- **Purpose**: This line displays the bar plot.
- **`plt.show()`:** This function displays the current figure (bar plot) that was created earlier.

### Summary:
- The code calculates the counts of unique authors and selects the top 10 authors with the most books.
- It creates a bar plot to visualize the number of books written by each of the top 10 authors.
- The bar plot is customized with a title, axis labels, and rotated x-axis labels for better readability.
- Finally, the bar plot is displayed using Matplotlib's `plt.show()` function.
"""

author_counts = Books['Book-Author'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=author_counts.index, y=author_counts.values, color='purple')
plt.title('Top 10 Authors with Most Books')
plt.xlabel('Author')
plt.ylabel('Number of Books')
plt.xticks(rotation=45)
plt.show()

"""# **Function to recommend books based on user input**


### `def recommend_books(query, n=5):`
- **Purpose**: This function takes a query (text input) and recommends a specified number (`n`) of similar books based on the query.
- **Parameters**:
  - `query`: The text query provided by the user.
  - `n=5`: The number of similar books to recommend (default is 5 if not specified).

### `query = preprocess(query)`
- **Purpose**: This line preprocesses the input query text using the `preprocess` function defined earlier.
- **`preprocess(query)`:** This function preprocesses the input query text, which may include tasks like lowercasing, removing punctuation, etc.

### `query_vector = vectorizer.transform([query])`
- **Purpose**: This line transforms the preprocessed query text into a TF-IDF vector using the same vectorizer (`vectorizer`) that was fitted earlier.
- **`vectorizer.transform([query])`:** This method transforms the preprocessed query text (`[query]` is a list containing the query) into a TF-IDF vector representation.

### `query_lsa = lsa.transform(query_vector)`
- **Purpose**: This line transforms the TF-IDF vector of the query into a lower-dimensional space using the dimensionality reduction pipeline (`lsa`).
- **`lsa.transform(query_vector)`:** This method applies the dimensionality reduction transformation (`lsa`) to the TF-IDF vector of the query.

### `similarity_scores = cosine_similarity(query_lsa, X_lsa).flatten()`
- **Purpose**: This line computes the cosine similarity between the query and all books in the dataset.
- **`cosine_similarity(query_lsa, X_lsa)`:** This function computes the cosine similarity between the transformed query vector (`query_lsa`) and all book vectors in the dataset (`X_lsa`).
- **`.flatten()`:** This method flattens the similarity scores into a 1D array.

### `top_indices = np.argsort(similarity_scores)[::-1][:n]`
- **Purpose**: This line finds the indices of the top `n` most similar books based on their cosine similarity scores.
- **`np.argsort(similarity_scores)`:** This function returns the indices that would sort the `similarity_scores` array in ascending order.
- **`[::-1]`:** This slicing reverses the sorted indices, so they are in descending order (highest similarity first).
- **`[:n]`:** This selects the first `n` indices, representing the top `n` most similar books.

### `recommended_books = Books.iloc[top_indices]`
- **Purpose**: This line retrieves the recommended books from the dataset based on the top indices found earlier.
- **`Books.iloc[top_indices]`:** This selects rows from the `Books` DataFrame using the indices of the top similar books.

### `return recommended_books`
- **Purpose**: This line returns the DataFrame containing the recommended books to the caller.

### Summary:
- The function preprocesses the input query text and transforms it into a TF-IDF vector.
- It then reduces the dimensionality of the query vector using the dimensionality reduction pipeline.
- Cosine similarity is computed between the query and all books in the dataset.
- The indices of the top `n` most similar books are determined based on their similarity scores.
- Finally, the function returns the DataFrame containing the recommended books.
"""

def recommend_books(query, n=5):
    query = preprocess(query)
    query_vector = vectorizer.transform([query])
    query_lsa = lsa.transform(query_vector)

    # Compute cosine similarity between the query and all books
    similarity_scores = cosine_similarity(query_lsa, X_train_lsa).flatten()

    # Get indices of top similar books
    top_indices = np.argsort(similarity_scores)[::-1][:n]

    # Get recommended books from the dataset
    recommended_books = Books.iloc[top_indices]
    return recommended_books

"""# **Chat interface**"""

# Chat interface
print("Welcome to the Book Recommendation System!")
print("Type 'exit' to quit.")
while True:
    user_input = input("\nLet's chat to know your taste: ")
    if user_input.lower() == 'exit':
        print("\nGoodbye!")
        break
    else:
        # Recommend books based on user input
        recommended_books = recommend_books(user_input)
        print("\nRecommended books:")
        for i, (title, author) in enumerate(zip(recommended_books['Book-Title'], recommended_books['Book-Author']), 1):
            print(f"{i}. {title} by {author}")